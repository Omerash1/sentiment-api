{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eb780",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Fine-tune DistilBERT for Sentiment Analysis\\n\",\n",
    "    \"This notebook fine-tunes DistilBERT on IMDb movie reviews for binary sentiment classification.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Setup and Imports\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from datasets import load_dataset\\n\",\n",
    "    \"from transformers import (\\n\",\n",
    "    \"    AutoTokenizer, \\n\",\n",
    "    \"    TFAutoModelForSequenceClassification,\\n\",\n",
    "    \"    DataCollatorWithPadding\\n\",\n",
    "    \")\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, classification_report\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set random seeds for reproducibility\\n\",\n",
    "    \"tf.random.set_seed(42)\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"GPU available: {tf.config.list_physical_devices('GPU')}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load IMDb Dataset\\n\",\n",
    "    \"print(\\\"Loading IMDb dataset...\\\")\\n\",\n",
    "    \"dataset = load_dataset(\\\"imdb\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Take 10% subset for faster training\\n\",\n",
    "    \"train_dataset = dataset[\\\"train\\\"].shuffle(seed=42).select(range(2500))  # 10% of 25k\\n\",\n",
    "    \"test_dataset = dataset[\\\"test\\\"].shuffle(seed=42).select(range(2500))    # 10% of 25k\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training samples: {len(train_dataset)}\\\")\\n\",\n",
    "    \"print(f\\\"Test samples: {len(test_dataset)}\\\")\\n\",\n",
    "    \"print(f\\\"Sample: {train_dataset[0]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load Pre-trained Model and Tokenizer\\n\",\n",
    "    \"model_name = \\\"distilbert-base-uncased\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loading tokenizer and model: {model_name}\\\")\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\",\n",
    "    \"model = TFAutoModelForSequenceClassification.from_pretrained(\\n\",\n",
    "    \"    model_name, \\n\",\n",
    "    \"    num_labels=2,\\n\",\n",
    "    \"    from_tf=False\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model loaded with {model.num_parameters()} parameters\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Tokenize Dataset\\n\",\n",
    "    \"def tokenize_function(examples):\\n\",\n",
    "    \"    return tokenizer(\\n\",\n",
    "    \"        examples[\\\"text\\\"],\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"        padding=False,  # We'll pad later with DataCollator\\n\",\n",
    "    \"        max_length=512\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Tokenizing datasets...\\\")\\n\",\n",
    "    \"tokenized_train = train_dataset.map(tokenize_function, batched=True)\\n\",\n",
    "    \"tokenized_test = test_dataset.map(tokenize_function, batched=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Tokenization complete!\\\")\\n\",\n",
    "    \"print(f\\\"Sample tokenized: {tokenized_train[0]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Convert to TensorFlow Format\\n\",\n",
    "    \"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\\\"tf\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Converting to TensorFlow datasets...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"tf_train_dataset = model.prepare_tf_dataset(\\n\",\n",
    "    \"    tokenized_train,\\n\",\n",
    "    \"    shuffle=True,\\n\",\n",
    "    \"    batch_size=16,\\n\",\n",
    "    \"    collate_fn=data_collator,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"tf_test_dataset = model.prepare_tf_dataset(\\n\",\n",
    "    \"    tokenized_test,\\n\",\n",
    "    \"    shuffle=False,\\n\",\n",
    "    \"    batch_size=16,\\n\",\n",
    "    \"    collate_fn=data_collator,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"TensorFlow datasets ready!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compile and Configure Model\\n\",\n",
    "    \"optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\\n\",\n",
    "    \"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n\",\n",
    "    \"metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\\n\",\n",
    "    \"\\n\",\n",
    "    \"model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Model compiled successfully!\\\")\\n\",\n",
    "    \"print(\\\"\\\\nModel Summary:\\\")\\n\",\n",
    "    \"model.summary()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Fine-tune Model\\n\",\n",
    "    \"print(\\\"Starting fine-tuning...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create callbacks\\n\",\n",
    "    \"callbacks = [\\n\",\n",
    "    \"    tf.keras.callbacks.EarlyStopping(\\n\",\n",
    "    \"        monitor='val_loss',\\n\",\n",
    "    \"        patience=1,\\n\",\n",
    "    \"        restore_best_weights=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train the model\\n\",\n",
    "    \"history = model.fit(\\n\",\n",
    "    \"    tf_train_dataset,\\n\",\n",
    "    \"    validation_data=tf_test_dataset,\\n\",\n",
    "    \"    epochs=2,\\n\",\n",
    "    \"    callbacks=callbacks,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training complete!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate Performance\\n\",\n",
    "    \"print(\\\"Evaluating model performance...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get predictions\\n\",\n",
    "    \"predictions = model.predict(tf_test_dataset)\\n\",\n",
    "    \"predicted_labels = np.argmax(predictions.logits, axis=-1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get true labels\\n\",\n",
    "    \"true_labels = []\\n\",\n",
    "    \"for batch in tf_test_dataset:\\n\",\n",
    "    \"    true_labels.extend(batch['labels'].numpy())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate metrics\\n\",\n",
    "    \"accuracy = accuracy_score(true_labels, predicted_labels)\\n\",\n",
    "    \"print(f\\\"\\\\nTest Accuracy: {accuracy:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Positive']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display training history\\n\",\n",
    "    \"print(f\\\"\\\\nTraining History:\\\")\\n\",\n",
    "    \"for epoch, (loss, acc, val_loss, val_acc) in enumerate(zip(\\n\",\n",
    "    \"    history.history['loss'],\\n\",\n",
    "    \"    history.history['sparse_categorical_accuracy'],\\n\",\n",
    "    \"    history.history['val_loss'],\\n\",\n",
    "    \"    history.history['val_sparse_categorical_accuracy']\\n\",\n",
    "    \")):\\n\",\n",
    "    \"    print(f\\\"Epoch {epoch+1}: loss={loss:.4f}, acc={acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save Model\\n\",\n",
    "    \"save_path = \\\"../sentiment_model\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saving model to {save_path}...\\\")\\n\",\n",
    "    \"model.save(save_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Also save the tokenizer for reference\\n\",\n",
    "    \"tokenizer.save_pretrained(save_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model and tokenizer saved successfully to {save_path}\\\")\\n\",\n",
    "    \"print(f\\\"Files in directory: {os.listdir(save_path)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test Inference\\n\",\n",
    "    \"print(\\\"Testing inference with saved model...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the saved model\\n\",\n",
    "    \"loaded_model = tf.keras.models.load_model(save_path)\\n\",\n",
    "    \"print(\\\"Model loaded successfully!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test predictions\\n\",\n",
    "    \"test_texts = [\\n\",\n",
    "    \"    \\\"This movie was absolutely fantastic! Great acting and storyline.\\\",\\n\",\n",
    "    \"    \\\"Terrible film, waste of time. Poor acting and boring plot.\\\",\\n\",\n",
    "    \"    \\\"It was okay, nothing special but not bad either.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for text in test_texts:\\n\",\n",
    "    \"    # Tokenize\\n\",\n",
    "    \"    inputs = tokenizer(\\n\",\n",
    "    \"        text,\\n\",\n",
    "    \"        max_length=512,\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"        padding=True,\\n\",\n",
    "    \"        return_tensors=\\\"tf\\\"\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Predict\\n\",\n",
    "    \"    outputs = loaded_model(inputs)\\n\",\n",
    "    \"    probabilities = tf.nn.softmax(outputs.logits, axis=-1).numpy()[0]\\n\",\n",
    "    \"    predicted_label = int(np.argmax(probabilities))\\n\",\n",
    "    \"    confidence = float(probabilities[predicted_label])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    sentiment = \\\"Positive\\\" if predicted_label == 1 else \\\"Negative\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nText: {text}\\\")\\n\",\n",
    "    \"    print(f\\\"Prediction: {sentiment} (confidence: {confidence:.4f})\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n✅ Model training and testing complete!\\\")\\n\",\n",
    "    \"print(\\\"🚀 You can now run the FastAPI server: uvicorn app.main:app --host 0.0.0.0 --port 8000\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
